{"cells":[{"cell_type":"markdown","metadata":{"id":"Cg047ujRBmtU"},"source":["# Decision Trees Exercise\n","In this exercise you will show that ID3 is sub-optimal. Implement a simple version of Decision Tree, and will then apply a Decision Tree classsifier on the MNIST hand written digits dataset that we already saw.\n"]},{"cell_type":"markdown","metadata":{"id":"osXAmT5y4iM8"},"source":["## 1. Suboptimality of ID3\n","Consider the following training set, where $\\mathcal{X} = \\{0, 1\\}^3$ and $\\mathcal{Y} =\\{0, 1\\}$:\n","\n","$$\n","\\begin{aligned}\n","((1, 1, 1), 1)\\\\\n","((1, 0, 0), 1)\\\\\n","((1, 1, 0), 0)\\\\\n","((0, 0, 1), 0)\n","\\end{aligned}\n","$$\n","\n","Suppose we wish to use this training set in order to build a decision tree of depth 2 (i.e. for each\n","input we are allowed to ask two questions of the form \"$x_i = 0$?\" before deciding on the label).\n","\n","1. Suppose we run the ID3 algorithm up to depth 2 (namely, we pick the root node and its\n","children according to the algorithm, but instead of keeping on with the recursion, we stop\n","and pick leaves according to the majority label in each subtree, once we reach depth 2). \n","Assume that the subroutine used to measure the quality of each feature is based on the information gain, and that if two features get the same score, one of them is picked arbitrarily. \n","Show that the training error of the resulting decision tree is at least 1/4.\n","2. Find a decision tree of depth 2, which attains zero training error.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Answer\n","\n","##### Question 1:\n","\n","Let's calculate Gain for each feature:\n","1. For 1st feature: 0.39\n","2. For 2nd feature: 0\n","3. For 3rd feature: 0\n","\n","\n","So, as a root we choose 1t feature and now we have next nodes:\n","\n","Feature 1 == 1, node A:\n","$$\n","\\begin{aligned}\n","((1, 1, 1), 1)\\\\\n","((1, 0, 0), 1)\\\\\n","((1, 1, 0), 0)\\\\\n","\\end{aligned}\n","$$\n","\n","Feature 1 == 0, node B:\n","$$\n","\\begin{aligned}\n","((0, 0, 1), 0)\n","\\end{aligned}\n","$$\n","\n","Node B is a final leaf with 0 entropy. \n","\n","For the node A, let's calculate gain for next 2 features:\n","1. For 2nd feature: 0.5\n","2. For 3nd feature: 0.5\n","\n","So, we could should one of them with the same gain.\n","\n","Let's now calculate the training error:\n","\n","In the final tree from this algorythm we gor 3 leafes: {0, 1, [1,0]} or {0,[1,0],0} in both cases 1 of 4 points is assigned incorrected, so the error is 1/4\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xC7Anlwu50XD"},"source":["#### Answer\n","\n","##### Question 2:\n","\n","1. Split by 2nd feature in subsets\n","\n","A:\n","$$\n","\\begin{aligned}\n","((1, 1, 1), 1)\\\\\n","((1, 1, 0), 0)\\\\\n","\\end{aligned}\n","$$\n","\n","B:\n","$$\n","\\begin{aligned}\n","((1, 0, 0), 1)\\\\\n","((0, 0, 1), 0)\n","\\end{aligned}\n","$$\n","\n","2. Then split A by the 3rd feature and B by the 1st feature and we got the set {1,0,1,0} with the train error is 0.\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{"id":"tLXpoHg64HlD"},"source":["## 2. Implementing Decision Tree From Scratch\n","In this exercise you will need to implement a simple version of Decision Tree from scratch. Your decision tree will handle **continuous input and output** (this should actually work also for binary input attributes).\n","\n","* Compelete the skeleton class below\n","  - `X` is a matrix of data values (rows are samples, columns are attributes)\n","  - `y` is a vector of corresponding target values\n","  - `min_leaf` is the minimal number of samples in each leaf node\n","  \n","* For splitting criterion, use either **\"Train Squared Error Minimization (Reduction in Variance)\"** or **\"Train Absolute Error Minimization\"** (choose one). Whatever you choose, make sure you implement the splitting point decision efficiently (in $O(nlgn)$ time).\n","\n","* The `predict` function will use mean of the target values in the leaf node matching each row of the given `X`. The result is a vector of predictions matching the number of rows in `X`.\n","\n","* To check your decision tree implementation, use the boston dataset (`from sklearn.datasets import load_boston`) split the data set into train and test using (`from sklearn.model_selection import train_test_split`)\n","\n","  - Use the following to estimate what are the best hyper parameters to use for your model\n","```\n","    for min_leaf in [1,5,10,100]:\n","      dt = DecisionTree(X, y, n, sz, min_leaf)\n","      mse = # mean square error over test set\n","      print(\"min_leaf:{0} --- oob mse: {1}\".format(min_leaf, mse))\n","```\n","  \n","  - Using your chosen hyperparameters as a final model, plot the predictions vs. true values of all the samples in the training set . Use something like:\n","  ```\n","  y_hat = dt.predict(X_train)  # forest is the chosen model\n","  plt.scatter(y_hat, y_test)\n","  ```"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.base import BaseEstimator, ClassifierMixin"]},{"cell_type":"code","execution_count":113,"metadata":{"id":"QA54r4DiQDkM"},"outputs":[],"source":["class DecisionTree(BaseEstimator,ClassifierMixin):\n","  def __init__(self, min_leaf):\n","    self.X = None\n","    self.y = None\n","    self.min_leaf = min_leaf\n","    self.right = None\n","    self.left = None\n","    self.feature = None\n","    self.feature_value = None\n","\n","  def fit(self, X, y):\n","    self.X = pd.DataFrame(X)\n","    self.y = np.array(y)\n","    # print('start fit')\n","    if (self.y.shape[0] <= self.min_leaf) or (np.unique(self.y).shape[0]==0):\n","      pass\n","    else:\n","      self.learn_the_feature()\n","      index_left = self.X.iloc[:,self.feature]  < self.feature_value\n","      index_right = self.X.iloc[:,self.feature]  >= self.feature_value\n","      left_tree  = self.X[index_left].loc[:,self.X.columns.values !=self.feature]\n","      right_tree = self.X[index_right].loc[:,self.X.columns.values !=self.feature]\n","      self.right = DecisionTree(self.min_leaf)\n","      self.right.fit(right_tree, self.y[index_right])\n","      # print('RIGHT')\n","      self.left = DecisionTree(self.min_leaf)\n","      self.left.fit(left_tree, self.y[index_left])\n","      # print('LEFT')\n","\n","\n","  def learn_the_feature(self):\n","    # print('start learn_the_feature')\n","    splits = []\n","    for ind in range(self.X.shape[1]):\n","      if np.var(self.X.iloc[:,ind]) == 0:\n","        # for those features that all the same for all data set some big number not to choose\n","        splits.append([1000,1000])\n","      else:\n","        splits.append(self.learn_best_split(self.X.iloc[:,ind].values))\n","    if len(splits) != 0:\n","      splits = np.array(splits)\n","      # print(splits)\n","      ind_min = splits[:,1].argmin()\n","      # print(ind_min, len(splits))\n","      # print(splits)\n","      self.feature = ind_min\n","      self.feature_value = splits[ind_min][0]\n","    else:\n","      print(self.X.shape)\n","\n","  \n","  def learn_best_split(self, x):\n","    # print('start learn_best_split')\n","    # x - is a vector from self.X\n","    # returns the best value to split\n","    values = []\n","    errors = []\n","    pd_xy = pd.DataFrame([x,self.y],index=['x','y']).T.sort_values('x')\n","    # print('sorted')\n","    for ind in range(1, pd_xy.shape[0]):\n","        value = pd_xy.iloc[ind]['x']\n","        if (value < pd_xy.iloc[ind-1]['x']) and (pd_xy.iloc[ind-1]['y'] != pd_xy.iloc[ind]['y']):\n","          print('if')\n","          left = pd_xy.loc[pd_xy.x < value]['y'].values\n","          right = pd_xy.loc[pd_xy.x >= value]['y'].values\n","          values.append(value)\n","          errors.append(self.split_error(left, right))\n","    values = np.array(values)\n","    errors = np.array(errors)\n","    if len(errors) > 0:\n","      result = [values[errors.argmin()], errors.min()]\n","    else:\n","      result = [pd_xy.iloc[ind]['x'], 100]\n","\n","    # choose the value that gives us the min split error\n","    return result\n","    \n","\n","  def split_error(self, left, right):\n","    print('start split_error')\n","    return np.var(left) + np.var(right)\n","\n","\n","  def predict(self, X):\n","    pd_X = pd.DataFrame(X)\n","    prediction = np.zeros(pd_X.shape[0], dtype='float64')\n","    if self.feature is None:\n","      # print(self.y.mean(), prediction)\n","      return np.full(pd_X.shape[0],self.y.mean(), dtype='float64')\n","      # return prediction.fill(self.y.mean())\n","    else:\n","      # prediction = []\n","      index_left = pd_X.iloc[:,self.feature]  < self.feature_value\n","      index_right = pd_X.iloc[:,self.feature]  >= self.feature_value\n","      left_tree  = pd_X[index_left].loc[:,pd_X.columns.values !=self.feature]\n","      right_tree = pd_X[index_right].loc[:,pd_X.columns.values !=self.feature]\n","      # print('left')\n","      # np.put(prediction, index_left, self.left.predict(left_tree))\n","      prediction[index_left] = self.left.predict(left_tree)\n","      # print('right')\n","      prediction[index_right] = self.right.predict(right_tree)\n","      return prediction\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":124,"metadata":{},"outputs":[],"source":["my_dt = DecisionTree(5)\n","my_dt.fit(X_train, y_train)\n","y_pred_train = my_dt.predict(X_train)\n","y_pred_test = my_dt.predict(X_test)\n","# matrix = confusion_matrix(y_test, y_pred)\n","# plt.imshow(matrix)\n","# print(f'The total accuracy: {np.trace(matrix)/ matrix.sum():.4f}')\n","print(f'Var of the error on TRAIN set: {np.var(y_pred_train - y_train) :.4}')\n","print(f'Var of the error on TEST set: {np.var(y_pred_test - y_test) :.4}')\n"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"data":{"text/plain":["array([ -1.2125    ,  -1.66666667,  -2.5       ,   6.67727273,\n","         4.77727273, -11.9       ,   2.08571429,  -6.3125    ,\n","         6.18      ,   9.        ,  -2.91428571,   4.58571429,\n","        -1.7       ,   4.18571429,   3.37727273, -10.22272727,\n","         1.75      ,   0.95      ,  -0.52      ,   2.78571429,\n","       -10.7       ,   2.37727273,   0.68571429,   0.67727273,\n","         0.28      ,  -2.61428571, -16.        ,   4.57727273,\n","         2.28      ,  -1.1       ,   2.28      ,   9.37727273,\n","        -0.62272727])"]},"execution_count":123,"metadata":{},"output_type":"execute_result"}],"source":["(y_pred-y_test)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"data":{"text/plain":["array([22.9, 23.4, 24.2, 19.3, 21.2, 30.8, 15.2, 28. , 15. , 24. , 20.2,\n","       12.7, 23.4, 13.1, 22.6, 36.2, 20. , 20.8, 21.7, 14.5, 35.4, 23.6,\n","       16.6, 25.3, 20.9, 19.9, 34.9, 21.4, 18.9, 22.8, 18.9, 16.6, 26.6])"]},"execution_count":117,"metadata":{},"output_type":"execute_result"}],"source":["y_test"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"data":{"text/plain":["(array([0.6, 0.6, 0.6, 0.6, 0.6]), 5)"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["ex = np.array([[1,2,3],[1,2,3],[1,4,5],[1,2,7],[1,2,8]])\n","ey = np.array([1,1,0,0,1])\n","\n","k = np.empty(ex.shape[0], dtype='float64')\n","# np.empty_like(pd_X.shape[0], dtype='float64')\n","k.fill(ey.mean())\n","k, ex.shape[0]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from sklearn.datasets import load_boston\n","boston = load_boston()\n"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(boston.data[:100], boston.target[:100], test_size=0.33, random_state=42)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["22.386567164179105"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["y_pred"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/tansla/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n","\n","    The Boston housing prices dataset has an ethical problem. You can refer to\n","    the documentation of this function for further details.\n","\n","    The scikit-learn maintainers therefore strongly discourage the use of this\n","    dataset unless the purpose of the code is to study and educate about\n","    ethical issues in data science and machine learning.\n","\n","    In this special case, you can fetch the dataset from the original\n","    source::\n","\n","        import pandas as pd\n","        import numpy as np\n","\n","\n","        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n","        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n","        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n","        target = raw_df.values[1::2, 2]\n","\n","    Alternative datasets include the California housing dataset (i.e.\n","    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n","    dataset. You can load the datasets as follows::\n","\n","        from sklearn.datasets import fetch_california_housing\n","        housing = fetch_california_housing()\n","\n","    for the California housing dataset and::\n","\n","        from sklearn.datasets import fetch_openml\n","        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n","\n","    for the Ames housing dataset.\n","    \n","  warnings.warn(msg, category=FutureWarning)\n"]},{"name":"stdout","output_type":"stream","text":["start fit\n","start learn_the_feature\n"]},{"ename":"IndexError","evalue":"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/var/folders/w6/3gxjz5p507nbml8f724sbh_r0000gn/T/ipykernel_30288/1593853068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_leaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mDT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mDT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mMSE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/var/folders/w6/3gxjz5p507nbml8f724sbh_r0000gn/T/ipykernel_30288/3830095964.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_the_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mindex_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0mindex_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/var/folders/w6/3gxjz5p507nbml8f724sbh_r0000gn/T/ipykernel_30288/3830095964.py\u001b[0m in \u001b[0;36mlearn_the_feature\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mind_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_min\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind_min\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"]}],"source":["from sklearn.datasets import load_boston\n","boston = load_boston()\n","X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.33, random_state=42)\n","to_test = np.array([1,5,10,100])\n","MSE = np.zeros(to_test.shape[0])\n","for i, min_leaf in enumerate(to_test):\n","  DT = DecisionTree(min_leaf)\n","  DT.fit(X_train, y_train)\n","  prediction = DT.predict(X_test)\n","  MSE[i] = np.mean((prediction - y_test)**2)\n","print(np.round(MSE))"]},{"cell_type":"code","execution_count":290,"metadata":{},"outputs":[{"data":{"text/plain":["0.0"]},"execution_count":290,"metadata":{},"output_type":"execute_result"}],"source":["X_train.pixel1.mean()"]},{"cell_type":"code","execution_count":287,"metadata":{},"outputs":[{"data":{"text/plain":["(array([[1, 2, 3],\n","        [1, 2, 3],\n","        [1, 4, 5],\n","        [1, 2, 7],\n","        [1, 2, 8]]),\n"," array([1, 1, 0, 0, 1]))"]},"execution_count":287,"metadata":{},"output_type":"execute_result"}],"source":["ex = np.array([[1,2,3],[1,2,3],[1,4,5],[1,2,7],[1,2,8]])\n","ey = np.array([1,1,0,0,1])\n","ex, ey"]},{"cell_type":"code","execution_count":221,"metadata":{},"outputs":[{"data":{"text/plain":["(5, 2)"]},"execution_count":221,"metadata":{},"output_type":"execute_result"}],"source":["index_left = self.X.iloc[:,self.feature]  < self.feature_value\n","left_tree  = self.X[index_left].loc[:,self.X.columns.values !=self.feature]\n","self.left = DecisionTree(left_tree, self.y[index_left], self.min_leaf)"]},{"cell_type":"code","execution_count":223,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1, 2, 3],\n","       [1, 2, 3],\n","       [1, 4, 5],\n","       [1, 2, 7],\n","       [1, 2, 8]])"]},"execution_count":223,"metadata":{},"output_type":"execute_result"}],"source":["ex2 = np.array([[1,2,3],[1,2,3],[1,4,5],[1,2,7],[1,2,8]])\n","ex2"]},{"cell_type":"code","execution_count":231,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":231,"metadata":{},"output_type":"execute_result"}],"source":["ex2[ex2[:,2].argmax()][0]"]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"data":{"text/plain":["array([3, 3, 5, 7, 8])"]},"execution_count":214,"metadata":{},"output_type":"execute_result"}],"source":["ex2.iloc[:,2].values"]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   x  y\n","0  3  1\n","1  3  1\n","2  5  0\n","3  7  0\n","4  8  1"]},"execution_count":206,"metadata":{},"output_type":"execute_result"}],"source":["ex[:,2]\n","pd_x = pd.DataFrame([ex[:,2],ey],index=['x','y']).T.sort_values('x')\n","pd_x"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd_x.loc[pd_x.x <= 2]['y'].values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for value in pd_x['x'].unique():\n","    left = pd_x.loc[pd_x.x < value]['y'].values\n","    right = pd_x.loc[pd_x.x >= value]['y'].values\n","    print(value, left, right)"]},{"cell_type":"code","execution_count":215,"metadata":{},"outputs":[],"source":["values = []\n","splits = []\n","pd_xy = pd.DataFrame([ex2.iloc[:,2].values,ey],index=['x','y']).T.sort_values('x')\n","for ind in range(1, pd_xy.shape[0]):\n","    value = pd_xy.iloc[ind]['x']\n","    if (value > pd_xy.iloc[ind-1]['x']) and (pd_xy.iloc[ind-1]['y'] != pd_xy.iloc[ind]['y']):\n","        left = pd_xy.loc[pd_xy.x < value]['y'].values\n","        right = pd_xy.loc[pd_xy.x >= value]['y'].values\n","\n","        values.append(value)\n","        splits.append( left.mean() + right.mean()-1)\n"]},{"cell_type":"code","execution_count":216,"metadata":{},"outputs":[{"data":{"text/plain":["([5, 8], [0.33333333333333326, 0.5])"]},"execution_count":216,"metadata":{},"output_type":"execute_result"}],"source":["values, splits"]},{"cell_type":"code","execution_count":205,"metadata":{},"outputs":[{"data":{"text/plain":["8"]},"execution_count":205,"metadata":{},"output_type":"execute_result"}],"source":["values[np.array(splits).argmax()]"]},{"cell_type":"markdown","metadata":{"id":"TF5TjNuvTKof"},"source":["## 3. Using Decision Tree for Digits Classification\n","Remeber the MNIST dataset used - you will now test the power of decision trees on this problem.\n","This time you are given a free hand in choosing the test and train set sizes, model parameters (such as gain function and constraints over the trees) and features (whether to use binary pixel values or the original continous gray value).\n","  - You can use `sklearn.tree.DecisionTreeClassifier`\n","- Once you are satisfied with the model parameters, plot the importance of each of the pixels to the final decision.\n","- Last, estimate the class assignment probabilities for all the correctly classified and misclassified examples in your test data.\n","- Discuss your results."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.datasets import fetch_openml\n","X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X.shape"]},{"cell_type":"code","execution_count":235,"metadata":{"id":"8-k9WpIV_n7Y","pycharm":{"name":"#%%\n"}},"outputs":[],"source":["# code and answer go here\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=317,stratify=y)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Baseline from NB exercise: The total accuracy: 0.8370"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtc = DecisionTreeClassifier()\n","dtc.fit(X_train, y_train)\n","y_pred = dtc.predict(X_test)\n","matrix = confusion_matrix(y_test, y_pred)\n","plt.imshow(matrix)\n","print(f'The total accuracy: {np.trace(matrix)/ matrix.sum():.4f}')\n","print(f'DecisionTreeClassifier Score on TRAIN set: {dtc.score(X_train, y_train):.4}')\n","print(f'DecisionTreeClassifier Score on TEST set: {dtc.score(X_test, y_test):.4}')\n","fi = dtc.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","plt.imshow(fi.reshape(28,28))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(np.where(fi>0.005,1,0).reshape(28,28))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtc.predict_proba(X_test).sum(axis=1).max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cheking for different values of feature importance how many features are there\n","a = 0.005\n","plt.plot(fi[fi>a])\n","print(fi[fi>a].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","parameters = {'max_features':(3,8,11, 20,32), 'max_depth': range(5,50)}\n","\n","model = DecisionTreeClassifier()\n","grid_GBC = GridSearchCV(model, parameters)\n","grid_GBC.fit(X_train, y_train)\n","\n","best_params = grid_GBC.best_params_\n","\n","print(\" Results from Grid Search \" )\n","print(f'The best estimator across ALL searched params on TRAIN set: {grid_GBC.best_estimator_}')\n","print(f'The best score across ALL searched params on TRAIN set: {grid_GBC.best_score_:.4f}')\n","print(f'The best parameters across ALL searched params on TRAIN set: {best_params}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtc = DecisionTreeClassifier(max_features=32, max_depth=16)\n","dtc.fit(X_train, y_train)\n","y_pred = dtc.predict(X_test)\n","matrix = confusion_matrix(y_test, y_pred)\n","plt.imshow(matrix)\n","print(f'The total accuracy: {np.trace(matrix)/ matrix.sum():.4f}')\n","print(f'DecisionTreeClassifier Score on TRAIN set: {dtc.score(X_train, y_train):.4}')\n","print(f'DecisionTreeClassifier Score on TEST set: {dtc.score(X_test, y_test):.4}')\n","fi = dtc.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.unique(dtc.predict_proba(X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.imshow(fi.reshape(28,28))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's binarize the data and train a new tree:\n","\n","model_param = 150\n","X_train_bool = np.where(X_train>model_param,1,0)\n","X_test_bool = np.where(X_test>model_param,1,0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtc = DecisionTreeClassifier()\n","dtc.fit(X_train_bool, y_train)\n","y_pred = dtc.predict(X_test_bool)\n","matrix = confusion_matrix(y_test, y_pred)\n","plt.imshow(matrix)\n","print(f'The total accuracy: {np.trace(matrix)/ matrix.sum():.4f}')\n","print(f'DecisionTreeClassifier Score on TRAIN set: {dtc.score(X_train_bool, y_train):.4}')\n","print(f'DecisionTreeClassifier Score on TEST set: {dtc.score(X_test_bool, y_test):.4}')\n","fi = dtc.feature_importances_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Cheking for different values of feature importance how many features are there\n","a = 0.02\n","plt.plot(fi[fi>a])\n","print(fi[fi>a].shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","parameters = {'max_features':range(5,35,3), 'max_depth': range(5,50,3)}\n","\n","model = DecisionTreeClassifier()\n","grid_GBC = GridSearchCV(model, parameters)\n","grid_GBC.fit(X_train_bool, y_train)\n","\n","best_params = grid_GBC.best_params_\n","\n","print(\" Results from Grid Search \" )\n","print(f'The best estimator across ALL searched params on TRAIN set: {grid_GBC.best_estimator_}')\n","print(f'The best score across ALL searched params on TRAIN set: {grid_GBC.best_score_:.4f}')\n","print(f'The best parameters across ALL searched params on TRAIN set: {best_params}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dtc = DecisionTreeClassifier(max_features=32, max_depth=38)\n","dtc.fit(X_train_bool, y_train)\n","y_pred = dtc.predict(X_test_bool)\n","matrix = confusion_matrix(y_test, y_pred)\n","plt.imshow(matrix)\n","print(f'The total accuracy: {np.trace(matrix)/ matrix.sum():.4f}')\n","print(f'DecisionTreeClassifier Score on TRAIN set: {dtc.score(X_train_bool, y_train):.4}')\n","print(f'DecisionTreeClassifier Score on TEST set: {dtc.score(X_test_bool, y_test):.4}')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"4afb1a9de92a1480aa03d8cf2eb92200404e379489b42821adaf7654ce4ec22a"}}},"nbformat":4,"nbformat_minor":0}
