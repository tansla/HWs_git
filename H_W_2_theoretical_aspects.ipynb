{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4WWyW4tM2Ir"
      },
      "source": [
        "# Deep Learning Theoretical Aspects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1ZXXN9lM2Is"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2qwj-IZM2Iw"
      },
      "source": [
        "Much of the power of neural networks comes from the nonlinearity that is inherited in activation functions.  \n",
        "Show that a network of N layers that uses a linear activation function can be reduced into a network with just an input and output layers.\n",
        "\n",
        "(Write down what is the output of two layers and use induction to claim for all layers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmDcfgIOM2Ix"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pLxCH5fp_ZmX"
      },
      "source": [
        "#### Answer:\n",
        "\n",
        "Let's assume that activation functions are linear that means we have some $f(x) = W*x +b$, where W is a matrix of the weight matrix, x is input layer, b is a bias and the result of this function give us a next layer.\n",
        "\n",
        "In case we have only 2 layers we will get:\n",
        "\n",
        "$x_1 = W_0*x_0 + b_0$\n",
        "$x_2 = W_1*x_1 + b_1 = W_1*W_0*x_0 + W_1*b_0 + b_1$\n",
        "\n",
        "the second expression is a linear function of the original layer with weights $W_1*W_0$ and bias $W_1*b_0 + b_1$ which means we do not need the 1st layer to calculate 2nd layer, we just need to adjust the weights and bias.\n",
        "\n",
        "We could extend this formula to include more layers and therefore show that linear activation functions will be not powerfull in NN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e-VlB4eM2Iz"
      },
      "source": [
        "### Derivatives of Activation Functions (30 points)\n",
        "Compute the derivative of these activation functions:\n",
        "\n",
        "1 Sigmoid\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*Vo7UFksa_8Ne5HcfEzHNWQ.png\" width=\"150\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bk-7BzFQM2I0"
      },
      "source": [
        "\n",
        "\n",
        "$$f'(t) = \\frac{d}{dt}(1+e^{-t})^{-1} = -(1+e^{-t})^{-2}\\frac{d}{dt}(1+e^{-t}) =-(1+e^{-t})^{-2} * -1*e^{-t} = (1+e^{-t})^{-2}*e^{-t} = \\frac{e^{-t}}{(1+e^{-t})^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0AiF6YjM2I3"
      },
      "source": [
        "2 Relu \n",
        "\n",
        "<img src=\"https://cloud.githubusercontent.com/assets/14886380/22743194/73ca0834-ee54-11e6-903f-a7efd247406b.png\" width=\"200\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BWTRtEX8M2I4"
      },
      "source": [
        "$$f'(x) = \\begin{cases} 0, &\\text{ if } x<0 \\\\ Ö¿1, &\\text{   if }x>0 \\end{cases}$$ \n",
        "\n",
        "And undefined for $x=0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tcbCKStM2I7"
      },
      "source": [
        "3 Softmax\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3\" width=\"250\">"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qb8zeNBM2I8"
      },
      "source": [
        "$$\\sigma(z_j) = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}$$\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial \\sigma(z_j)}{\\partial z_j} = \\frac{\\frac{\\partial}{\\partial z_j} e^{z_j} \\cdot \\sum_{k=1}^{K} e^{z_k} - e^{z_j} \\cdot \\frac{\\partial}{\\partial z_j} \\sum_{k=1}^{K} e^{z_k}}{(\\sum_{k=1}^{K} e^{z_k})^2}=\n",
        "\\frac{e^{z_j} \\cdot \\sum_{k=1}^{K} e^{z_k} - e^{z_j} \\cdot (e^{z_j} + \\sum_{k\\neq j} e^{z_k})}{(\\sum_{k=1}^{K} e^{z_k})^2}=\n",
        "\\frac{e^{z_j} \\cdot (\\sum_{k=1}^{K} e^{z_k} - e^{z_j})}{(\\sum_{k=1}^{K} e^{z_k})^2}$$\n",
        "\n",
        "\n",
        "$$\\frac{\\partial \\sigma(z_j)}{\\partial z_j} = \\sigma(z_j) (1 - \\sigma(z_j))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRE-pv-zM2I-"
      },
      "source": [
        "### Back Propagation (30 points)\n",
        "Use the chain rule and backprop (also called the generalized delta rule) to compute the partial derivatives for these computations (i.e., dz/dx1, dz/dx1, dz/dx3):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJZ_0mWM2JA"
      },
      "source": [
        "```\n",
        "z = x1 + 5*x2 - 3*x3^2\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$z = x_1 + 5*x_2 - 3*x_3^2$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial z}{\\partial x_1} = 1$$\n",
        "$$\\frac{\\partial z}{\\partial x_2} = 5$$\n",
        "$$\\frac{\\partial z}{\\partial x_3} = -6x_3$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgwnBRJgM2JD"
      },
      "source": [
        "```\n",
        "z = x1*(x2-4) + exp(x3^2) / 5*x4^2\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$z = x_1*(x_2-4) + \\frac{e^{x_3^2}}{ 5*x_4^2}$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ1igYpxM2JE"
      },
      "source": [
        "$$\\frac{\\partial z}{\\partial x_1} = x_2 - 4$$\n",
        "$$\\frac{\\partial z}{\\partial x_2} = x_1$$\n",
        "$$\\frac{\\partial z}{\\partial x_3} = \\frac{2x_3e^{x_3^2}}{5x_4^2}$$\n",
        "$$\\frac{\\partial z}{\\partial x_4} = -\\frac{2exp(x_3^2)}{5x_4^3}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIx61WAM2JI"
      },
      "source": [
        "```\n",
        "z = 1/x3 + exp( (x1+5*(x2+3)) ^2 )\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$z = \\frac{1}{x_3} + e^{(x_1+5*(x_2+3)) ^2}$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\\frac{\\partial z}{\\partial x_1} = 10(x_1 + 5(x_2+3))e^{(x_1+5(x_2+3))^2}$$\n",
        "$$\\frac{\\partial z}{\\partial x_2} = 50(x_1 + 5(x_2+3))e^{(x_1+5(x_2+3))^2}$$\n",
        "$$\\frac{\\partial z}{\\partial x_3} = -\\frac{1}{x_3^2}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvTBLm44M2Jq"
      },
      "source": [
        "### Puppy or bagel? (20 points)\n",
        "We've seen in class the (hopefully) funny examples of challenging images (Chihuahua or muffin, puppy or bagel etc.). \n",
        "\n",
        "Let's say you were asked by someone to find more examples like that. You are able to call the 3 neural networks that won the recent ImageNet challenges, and get their predictions (the entire vector of probabilities for the 1000 classes).  \n",
        "\n",
        "Describe methods that might assist you in finding more examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsTYNPDvM2Jr"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JF9a_kQdqlk"
      },
      "source": [
        "### Convolution (20 points)\n",
        "Consider the following convolution filters:\n",
        "```python\n",
        "k1 = [ [0 0 0], [0 1 0], [0 0 0] ]\n",
        "k2 = [ [0 0 0], [0 0 1], [0 0 0] ]\n",
        "k3 = [ [-1-1 -1], [-1 8 -1], [-1 -1 -1] ]\n",
        "k4 = [ [1 1 1], [1 1 1], [1 1 1] ] / 9\n",
        "```\n",
        "\n",
        "Can you guess what each of them computes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzP36pi3eGvn"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
